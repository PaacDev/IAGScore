\capitulo{3}{Conceptos teóricos}

Este capítulo presenta los fundamentos teóricos necesarios para comprender los principales componentes que sustentan la aplicación desarrollada. Se abordan los conceptos de modelos de lenguaje a gran escala (\textit{LLM}), técnicas de ingeniería de \textit{prompts}, el uso de rúbricas como herramienta para la evaluación objetiva y la integración de mecanismos automáticos para la corrección asistida de tareas.

\section{Modelos de lenguaje a gran escala (\textit{LLM})}

Los modelos de lenguaje a gran escala (\textit{LLM}, por sus siglas en inglés) son sistemas de 
inteligencia artificial diseñados para modelar y procesar el lenguaje humano. 
Estos modelos forman parte del campo del \textit{procesamiento de lenguaje natural (PLN)}\cite{ibm:pln2024}, 
una rama de la inteligencia artificial que estudia cómo las máquinas pueden comprender, 
interpretar y generar texto o habla en el lenguaje humano.

Se denominan ``grandes'' o ``a gran escala'' porque están entrenados con volúmenes 
masivos de texto, y tienen una arquitectura muy grande capaz de capturar matices 
lingüísticos complejos que los modelos más pequeños no pueden\cite{wiki:llm}.

\subsection*{Arquitectura \textit{Transformer}}

La arquitectura \textit{Transformer}~\cite{datacamp:transformers2024} es la base de la mayoría de los modelos de lenguaje actuales (\textit{LLMs}). A continuación, se describe de forma simplificada su funcionamiento:

\begin{itemize}
    \item \textbf{Entrada (de palabras a números)}: el modelo toma una frase y convierte cada palabra en una representación numérica comprensible para la máquina. Este proceso se conoce como \textit{embedding}.
    
    \item \textbf{Mecanismo de atención}: el \textit{Transformer} identifica qué partes del texto son más relevantes mediante la atención (\textit{self-attention}). Esta técnica permite al modelo analizar todas las palabras al mismo tiempo y establecer qué relaciones son más significativas.
    
    \item \textbf{Capas de procesamiento}: la información se pasa por múltiples capas de redes neuronales, que realizan cálculos para refinar la comprensión del texto.
    
    \item \textbf{Salida (de números a texto)}: finalmente, el modelo convierte los resultados numéricos en palabras para generar una respuesta o texto coherente.
\end{itemize}

La arquitectura se compone de dos bloques principales:

\begin{itemize}
    \item \textbf{Codificador (\textit{encoder})}: procesa y comprende el texto de entrada. Está formado por varias capas que combinan atención y redes neuronales para capturar el significado de la secuencia.

    \item \textbf{Decodificador (\textit{decoder})}: genera el texto de salida utilizando la información del codificador, palabra por palabra, teniendo en cuenta tanto el contexto como el contenido generado previamente.
\end{itemize}

\imagen{transformer}{Diagrama del modelo \textit{Transformer}~\cite{deeprevision2023}}{1}

\subsection*{Aplicación en IAGScore}

En este contexto, los \textit{LLMs} no solo representan una tecnología de vanguardia, 
sino que constituyen el núcleo funcional de la herramienta \textbf{IAGScore}. 
Gracias a su capacidad para interpretar instrucciones en lenguaje natural y analizar fragmentos de código, los \textit{LLMs} permiten automatizar tareas que tradicionalmente requerían intervención humana, 
como la evaluación detallada de ejercicios de programación y la generación de retroalimentación 
contextualizada.

Una característica especialmente valiosa para el proyecto es la posibilidad de guiar el 
comportamiento del modelo mediante \textit{prompts} bien diseñados, lo que permite adaptar 
las respuestas a rúbricas personalizadas o criterios específicos definidos por el profesorado.

\section{Prompt engineering}

El \textit{prompt engineering} es una técnica dentro del campo del procesamiento de lenguaje natural que consiste en diseñar cuidadosamente las instrucciones que se proporcionan a un modelo de lenguaje (\textit{LLM}) para obtener respuestas más precisas, relevantes o útiles.

Dado que los \textit{LLM} no tienen comprensión real del contexto, el modo en que se les formula una entrada (\textit{prompt}) influye significativamente en la calidad y precisión de la respuesta. Por ello, estructurar correctamente el mensaje de entrada se convierte en un factor clave para guiar el comportamiento del modelo.

Aunque en este proyecto no se ha aplicado de forma intensiva, se han realizado pruebas con distintas formulaciones de entrada para verificar cómo pequeños cambios en el \textit{prompt} afectan la salida del modelo. Por ejemplo, el uso de frases más explícitas o el establecimiento claro del rol del modelo (e.g.,``Actúa como profesor universitario'') puede mejorar la coherencia y adecuación de las respuestas al evaluar tareas~\cite{web:promptingguide}.

Existen técnicas avanzadas dentro del \textit{prompt engineering}, como:

\begin{itemize}
    \item \textbf{\textit{Zero-shot prompting}}: el modelo genera una respuesta sin ejemplos previos, 
	confiando solo en la instrucción.
    \item \textbf{\textit{Few-shot prompting}}: se incluyen uno o varios ejemplos dentro del \textit{prompt} para que el modelo aprenda el formato o estilo deseado.
    \item \textbf{\textit{Chain-of-thought prompting}}: se induce al modelo a razonar paso a paso para 
	tareas complejas.
    \item \textbf{\textit{Role prompting}}: se establece un rol específico para el modelo, como ``profesor universitario''.
\end{itemize}

Aunque estas estrategias no se han utilizado en profundidad en este trabajo, su conocimiento es relevante para posibles mejoras futuras del sistema, especialmente si se busca afinar el comportamiento del modelo en escenarios educativos específicos.

\section{Rúbricas}

Las rúbricas son instrumentos de evaluación que describen criterios específicos para valorar el desempeño de una tarea o actividad. Se componen generalmente de una serie de dimensiones o aspectos a evaluar, junto con niveles de desempeño definidos para cada una de ellas, que permiten asignar puntuaciones de forma objetiva y transparente.

En el ámbito educativo, las rúbricas son ampliamente utilizadas para evaluar tareas abiertas, como ensayos, proyectos, presentaciones o, en el caso de este proyecto, ejercicios de programación. Su estructura ayuda tanto a los evaluadores como a los estudiantes, ya que clarifica las expectativas y facilita la retroalimentación formativa~\cite{wiki:rubrica}.

Una rúbrica típica incluye:

\begin{itemize}
    \item \textbf{Criterios de evaluación}: aspectos concretos que se desean valorar (e.g., legibilidad del código, eficiencia, cumplimiento de requisitos).
    \item \textbf{Niveles de desempeño}: descripciones cualitativas (y en ocasiones cuantitativas) que definen grados de calidad o cumplimiento.
    \item \textbf{Puntuaciones}: valores numéricos o etiquetas asociadas a cada nivel.
\end{itemize}

En este proyecto, las rúbricas se utilizan como entrada esencial para el modelo de lenguaje. El usuario define una rúbrica en formato \texttt{markdown}, que posteriormente es 
añadida al \textit{prompt} y enviada al \textit{LLM}, junto con el código del estudiante. El modelo analiza el código y genera una evaluación basada en los criterios proporcionados, simulando el comportamiento de un evaluador humano.

Este enfoque permite automatizar evaluaciones personalizadas, intentando mantener un grado alto de coherencia con las intenciones pedagógicas del docente. Además, al permitir 
reestructurar y reutilizar rúbricas fácilmente, se favorece la escalabilidad y adaptabilidad del sistema a distintos contextos de enseñanza.

\section{Corrección automática de tareas de programación}

La corrección automática de tareas de programación es una técnica que busca evaluar de manera sistemática y eficiente el código fuente entregado por estudiantes o usuarios, mediante herramientas informáticas capaces de analizar y valorar distintos aspectos del mismo. Este proceso puede incluir verificación sintáctica, evaluación funcional, análisis estático, comparación con soluciones de referencia o, como en este proyecto, análisis mediante modelos de lenguaje a gran escala.

El enfoque adoptado se basa en el uso de modelos \textit{LLM} para comprobar no solo si un programa funciona correctamente, sino que pueden emitir juicios cualitativos sobre aspectos relacionados con la rúbrica definida por el usuario. 
Estos aspectos pueden ser:

\begin{itemize}
    \item Claridad y estilo del código.
    \item Cumplimiento de requisitos.
    \item Organización y estructura lógica.
    \item Correcto uso de estructuras del lenguaje de programación.
    \item Comentarios y documentación.
\end{itemize}

Para que el modelo realice esta evaluación, se le proporciona una entrada estructurada que combina:

\begin{enumerate}
    \item Un \textit{prompt} que contextualiza la tarea del modelo (e.g., ``Eres un profesor que debe evaluar este código según los siguientes criterios...'').
    \item La rúbrica definida por el usuario, en formato estructurado.
    \item El código fuente a evaluar.
\end{enumerate}

Esta combinación se ensambla en una cadena de texto que es enviada al modelo a través de una tarea en segundo plano. El resultado es una evaluación 
textual generada por el \textit{LLM}, que puede incluir puntuaciones, observaciones, comentarios generales y sugerencias de mejora.

\imagen{ensamblado}{Diagrama ensamblado, envío a modelo y respuesta}{1}

Este tipo de corrección presenta ventajas significativas:

\begin{itemize}
    \item \textbf{Escalabilidad}: se pueden corregir grandes volúmenes de tareas en poco tiempo.
    \item \textbf{Consistencia}: las evaluaciones tienden a ser coherentes entre tareas similares, al depender de un único sistema de criterios.
    \item \textbf{Flexibilidad}: permite adaptar el comportamiento del modelo mediante ajustes de parámetros y personalización de \textit{prompts}.
\end{itemize}

Sin embargo, también existen limitaciones, como la falta de ejecución real del código (a menos que se combine con otros análisis) o la posibilidad de interpretaciones ambiguas por parte del modelo si el \textit{prompt} no está bien diseñado. Por ello, se contempla esta herramienta como un apoyo a la evaluación docente, no necesariamente como un sustituto total.

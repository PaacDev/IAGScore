\capitulo{3}{Conceptos teóricos}

En este apartado se describen los componentes principales que hacen posible la evaluación 
automática de tareas mediante modelos de lenguaje de gran tamaño (LLM). 

En primer lugar, se explica en qué consisten los LLM y cómo su arquitectura basada en 
transformadores permite comprender y generar texto de forma avanzada. A continuación, 
se introduce LangChain como un \textit{framework} que facilita la integración de estos 
modelos en aplicaciones complejas, y se analiza el uso de Ollama como herramienta para 
ejecutar los modelos localmente, garantizando privacidad y control sobre los datos.

Además, se detalla el papel de Redis y Celery, que permiten gestionar tareas de forma 
asíncrona y eficiente dentro del sistema, especialmente útil cuando el procesamiento de 
las tareas requiere tiempo o recursos considerables.

Finalmente, se menciona el concepto de \textit{prompt engineering}, una técnica que permite 
mejorar la calidad de las respuestas generadas por los modelos optimizando la forma en que 
se les formula la entrada, aspecto que puede tener impacto directo en la efectividad del 
sistema desarrollado.

\section{Modelos de lenguaje grandes (LLM)}

Los modelos de lenguaje grandes (LLM, por sus siglas en inglés) son sistemas de 
inteligencia artificial diseñados para modelar y procesar el lenguaje humano. 
Estos modelos forman parte del campo del \textit{procesamiento de lenguaje natural} (PLN), 
una rama de la inteligencia artificial que estudia cómo las máquinas pueden comprender, 
interpretar y generar texto o habla en lenguaje humano.

Se denominan 'grandes' porque están entrenados con volúmenes 
masivos de texto, y tienen una arquitectura muy grande capaz de capturar matices 
lingüísticos complejos que los modelos más pequeños no pueden.

La gran mayoría de LLMs actuales se basan en la arquitectura Transformer, propuesta por 
primera vez en el artículo ``Attention Is All You Need''~\cite{vaswani2017}. 
contruyendo una arquitectura basada en mecanismos de atención, sin recurrir a 
redes recurrentes o convolucionales.

El mecanismo de atención permite al modelo ponderar la importancia relativa de cada palabra 
en la entrada, considerando su relación con todas las demás en la secuencia. Esto le permite 
capturar dependencias a largo plazo y relaciones semánticas complejas, incluso entre palabras 
que están muy separadas en el texto.

\section{Ollama}

Ollama es una herramienta que permite ejecutar modelos de lenguaje grandes de forma local, 
facilitando así la utilización de LLMs sin necesidad de conectarse a servicios en la nube. 
Esto representa una ventaja significativa en términos de privacidad, eficiencia y control 
sobre el entorno de ejecución.

Además de simplificar la instalación y gestión de modelos como LLaMA, Mistral o Phi-4, 
Ollama ofrece una interfaz sencilla para ejecutar estos modelos desde la línea de comandos 
o integrarlos en aplicaciones a través de bibliotecas específicas como LangChain. 

En este proyecto, Ollama ha sido clave para ejecutar LLMs directamente en el entorno local
evitando el uso de servicios externos.

Una de las características más útiles de Ollama es que permite configurar distintos parámetros 
del modelo, lo que facilita su adaptación a diferentes necesidades o contextos. 
En particular, en este proyecto se permite configurar los siguientes parámetros:

\begin{itemize}
	\item \textbf{Temperatura}: Controla la aleatoriedad de las respuestas generadas. Un valor bajo 
		produce respuestas más predecibles, mientras que un valor alto genera respuestas 
		más creativas y variadas.
	\item \textbf{Top-p}: También conocido como muestreo de núcleo, este parámetro ajusta la
		probabilidad acumulativa de las palabras seleccionadas. Un valor bajo limita la 
		selección a las palabras más probables, mientras que un valor alto permite una 
		mayor diversidad en las respuestas.
	\item \textbf{Top-k}: Este parámetro limita el número de palabras candidatas a considerar
		durante la generación de texto. Un valor bajo restringe la selección a las palabras 
		más probables, mientras que un valor alto permite una mayor variedad en las 
		respuestas generadas.
\end{itemize}

Además, Ollama facilita la gestión de versiones de los modelos, permitiendo a los usuarios
probar diferentes versiones de un mismo modelo o incluso modelos alternativos sin complicaciones.

\section{LangChain}

LangChain es un \textit{framework} diseñado para facilitar la creación de aplicaciones 
que integran modelos de lenguaje con diversas fuentes de datos, herramientas externas 
y flujos de trabajo. Aunque su potencial es muy amplio, en este proyecto se ha 
utilizado de forma básica para conectar modelos de lenguaje locales, 
gestionados con Ollama, e integrarlos de manera sencilla dentro del entorno de desarrollo 
en Python.

En concreto, se ha utilizado el módulo \texttt{OllamaLLM} de \texttt{langchain\_ollama}, 
que permite invocar modelos como LLaMA o Mistral localmente sin necesidad de APIs externas. 
Esta integración facilita el acceso a modelos avanzados sin comprometer la privacidad de 
los datos ni depender de servicios en la nube.

LangChain ofrece también funcionalidades más avanzadas como el manejo de memoria conversacional, 
agentes que interactúan con múltiples herramientas, y la orquestación de cadenas de prompts, 
aunque estas características no han sido necesarias en el alcance actual del proyecto.

\section{Gestión de tareas asíncronas con Celery y Redis}

En este proyecto se ha utilizado \textbf{Celery}, una librería de Python para la ejecución de
tareas en segundo plano y procesamiento distribuido, con el objetivo de gestionar de forma 
eficiente las correcciones automáticas enviadas por los estudiantes. Su integración 
permite desacoplar el proceso de corrección del flujo principal de la aplicación, 
evitando bloqueos o tiempos de espera innecesarios en la interfaz de usuario.

Esto resulta especialmente útil en entornos donde se espera una
alta concurrencia de usuarios o cuando el procesamiento de texto puede requerir 
tiempos de cómputo significativos, como es el caso del uso de LLMs.

Para la gestión de mensajes entre la aplicación y Celery se ha empleado \textbf{Redis}, 
un sistema de almacenamiento en memoria que actúa como \textit{message broker}. Redis 
permite almacenar las tareas en una cola, que posteriormente son recogidas y ejecutadas 
por los \textit{workers} de Celery.

El flujo básico de trabajo es el siguiente:
\begin{enumerate}
    \item El usuario envía una tarea desde la aplicación.
    \item Esta tarea se encola en Redis como un mensaje.
    \item Celery detecta la nueva tarea en la cola y la ejecuta, utilizando un modelo de lenguaje (LLM) gestionado por Ollama.
    \item Una vez completada la corrección, la respuesta se almacena para ser presentada al usuario.
\end{enumerate}

Este enfoque ofrece varias ventajas:
\begin{itemize}
    \item Permite procesar múltiples tareas en paralelo.
    \item Mejora la escalabilidad del sistema.
    \item Reduce la carga directa sobre el servidor principal.
    \item Aumenta la capacidad de respuesta de la interfaz.
\end{itemize}

\section{Prompt engineering}

El \textit{prompt engineering} es una técnica dentro del campo del procesamiento de lenguaje natural
que consiste en diseñar cuidadosamente las instrucciones que se proporcionan a un modelo de 
lenguaje (LLM) para obtener respuestas más precisas, relevantes o útiles.

Dado que los LLM no tienen comprensión real del contexto, el modo en que se les formula una 
entrada (prompt) influye significativamente en la calidad y pertinencia de la respuesta. 
Por ello, estructurar correctamente el mensaje de entrada se convierte en un factor clave 
para guiar el comportamiento del modelo.

Aunque en este proyecto no se ha aplicado de forma intensiva, se han realizado pruebas con distintas 
formulaciones de entrada para verificar cómo pequeños cambios en el prompt afectan la salida del 
modelo. Por ejemplo, el uso de frases más explícitas o el establecimiento claro del rol del modelo 
(e.g.,``Actúa como profesor universitario'') puede mejorar la coherencia y adecuación de las 
respuestas al evaluar tareas~\cite{web:promptingguide}.

Existen técnicas avanzadas dentro del \textit{prompt engineering}, como:

\begin{itemize}
    \item \textbf{Few-shot prompting}: Se incluyen uno o varios ejemplos dentro del prompt para 
	que el modelo aprenda el formato o estilo deseado.
    \item \textbf{Zero-shot prompting}: El modelo genera una respuesta sin ejemplos previos, 
	confiando solo en la instrucción.
    \item \textbf{Chain-of-thought prompting}: Se induce al modelo a razonar paso a paso para 
	tareas complejas.
    \item \textbf{Role prompting}: Se establece un rol específico para el modelo, como ``profesor universitario''.
\end{itemize}

Aunque estas estrategias no se han utilizado en profundidad en este trabajo, su conocimiento es relevante 
para posibles mejoras futuras del sistema, especialmente si se busca afinar el comportamiento del 
modelo en escenarios educativos específicos.


\subsubsection{Subsubsecciones}

Y subsecciones. 


\section{Referencias}

Las referencias se incluyen en el texto usando cite~\cite{wiki:latex}. Para citar webs, artículos o libros~\cite{koza92}, si se desean citar más de uno en el mismo lugar~\cite{bortolot2005, koza92}.


\section{Imágenes}

Se pueden incluir imágenes con los comandos standard de \LaTeX, pero esta plantilla dispone de comandos propios como por ejemplo el siguiente:

\imagen{escudoInfor}{Autómata para una expresión vacía}{.5}



\section{Listas de items}

Existen tres posibilidades:

\begin{itemize}
	\item primer item.
	\item segundo item.
\end{itemize}

\begin{enumerate}
	\item primer item.
	\item segundo item.
\end{enumerate}

\begin{description}
	\item[Primer item] más información sobre el primer item.
	\item[Segundo item] más información sobre el segundo item.
\end{description}
	
\begin{itemize}
\item 
\end{itemize}

\section{Tablas}

Igualmente se pueden usar los comandos específicos de \LaTeX o bien usar alguno de los comandos de la plantilla.

\tablaSmall{Herramientas y tecnologías utilizadas en cada parte del proyecto}{l c c c c}{herramientasportipodeuso}
{ \multicolumn{1}{l}{Herramientas} & App AngularJS & API REST & BD & Memoria \\}{ 
HTML5 & X & & &\\
CSS3 & X & & &\\
BOOTSTRAP & X & & &\\
JavaScript & X & & &\\
AngularJS & X & & &\\
Bower & X & & &\\
PHP & & X & &\\
Karma + Jasmine & X & & &\\
Slim framework & & X & &\\
Idiorm & & X & &\\
Composer & & X & &\\
JSON & X & X & &\\
PhpStorm & X & X & &\\
MySQL & & & X &\\
PhpMyAdmin & & & X &\\
Git + BitBucket & X & X & X & X\\
Mik\TeX{} & & & & X\\
\TeX{}Maker & & & & X\\
Astah & & & & X\\
Balsamiq Mockups & X & & &\\
VersionOne & X & X & X & X\\
} 
